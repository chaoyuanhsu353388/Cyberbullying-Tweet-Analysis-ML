# -*- coding: utf-8 -*-
"""cyberbullying_tweets_python_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oKOktldME3eqXXGY_9rkHY_LZuAsuLQ7
"""

import re, nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('omw-1.4')
nltk.download('wordnet')
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
import joblib

# Reading dataset as dataframe
df = pd.read_csv("cyberbullying_tweets.csv")
pd.set_option('display.max_colwidth', None) # Setting this so we can see the full content of cells
pd.set_option('display.max_columns', None) # to make sure we can see all the columns in output window

"""# Data Preparation"""

df.info()
df.shape

# Check any null values
df.isnull().sum()

# Check any duplicate values
df['tweet_text'].duplicated().sum()

# Drop Duplicate values
df.drop_duplicates('tweet_text', inplace=True, ignore_index=True)
df['tweet_text'].duplicated().sum()

# Check the balance of cyberbullying_type
df['cyberbullying_type'].value_counts()

df['cyberbullying_type'].unique()

# Other_cyberbullying is too broad and generic, it would affect the overall prediction
df = df[df["cyberbullying_type"] != "other_cyberbullying"]
df['cyberbullying_type'].unique()

# Lebal encoding
cyberbullying_type = ['not_cyberbullying', 'gender', 'religion', 'age', 'ethnicity']
encoding_dict = {'not_cyberbullying':0, 'gender':1, 'religion':1, 'age':1, 'ethnicity':1}

df['cyberbullying_type'] = df['cyberbullying_type'].map(encoding_dict)
df.info()

"""# Cleaning Tweets"""

def cleaner(tweet):

    # removing HTML entities such as ‘&amp’,’&quot’,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'
    soup = BeautifulSoup(tweet, 'lxml')
    souped = soup.get_text()

    # substituting @mentions, urls, etc with whitespace
    re1 = re.sub(r"(@|http://|https://|www|\\x)\S*", " ", souped)

    # substituting any non-alphabetic character that repeats one or more times with whitespace
    re2 = re.sub("[^A-Za-z]+"," ", re1)

    tokens = nltk.word_tokenize(re2)
    lower_case = [t.lower() for t in tokens]

    stop_words = set(stopwords.words('english'))
    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))

    wordnet_lemmatizer = WordNetLemmatizer()
    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]
    return lemmas

df['cleaned_tweet'] = df.tweet_text.apply(cleaner)

# removing rows with cleaned tweets of length 0
df = df[df['cleaned_tweet'].map(len) > 0]

print("Printing top 5 rows of dataframe showing original and cleaned tweets....")
print(df[['tweet_text','cleaned_tweet']].head())

# Saving cleaned tweets to csv
df.to_csv('cyberbullying_tweets_cleaned.csv', index=False)

# joining tokens to create strings. TfidfVectorizer does not accept tokens as input
df['cleaned_tweet'] = [" ".join(row) for row in df['cleaned_tweet'].values]
data = df['cleaned_tweet']
Y = df['cyberbullying_type']

# target column

# min_df=.00015 means that each ngram (unigram, bigram, & trigram) must be present in at least 30 documents for it to be considered as a token (200000*.00015=30). This is a clever way of feature engineering
tfidf = TfidfVectorizer(min_df=.00015, ngram_range=(1,3))
tfidf.fit(data)

# learn vocabulary of entire data

# creating tfidf values
data_tfidf = tfidf.transform(data)
pd.DataFrame(pd.Series(tfidf.get_feature_names_out())).to_csv('vocabulary.csv', header=False, index=False)
print("Shape of tfidf matrix: ", data_tfidf.shape)

"""# Support Vector Classifier"""

# Implementing Support Vector Classifier

# kernel = 'linear' and C = 1
model = LinearSVC()

# Running cross-validation

# 10-fold cross-validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
scores=[]
iteration = 0
for train_index, test_index in kf.split(data_tfidf, Y):
    iteration += 1
    print("Iteration ", iteration)

    # Ensure train and test indices are aligned with DataFrame
    X_train, Y_train = data_tfidf[train_index], Y.iloc[train_index] # Use iloc to access by position
    X_test, Y_test = data_tfidf[test_index], Y.iloc[test_index] # Use iloc to access by position

    # Fitting SVC
    model.fit(X_train, Y_train) # Use the defined 'model' variable
    Y_pred = model.predict(X_test) # Use the defined 'model' variable

    # Calculating accuracy
    score = metrics.accuracy_score(Y_test, Y_pred)
    print("Cross-validation accuracy: ", score)

    # appending cross-validation accuracy for each iteration
    scores.append(score)
svc_mean_accuracy = np.mean(scores)
print("Mean cross-validation accuracy: ", svc_mean_accuracy)

"""# Naive Bayes Classifier"""

nbc_clf = MultinomialNB()

# Running cross-validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation
scores=[]
iteration = 0
for train_index, test_index in kf.split(data_tfidf, Y):
    iteration += 1
    print("Iteration ", iteration)

    # Use .iloc to select rows by position
    X_train, Y_train = data_tfidf[train_index], Y.iloc[train_index]
    X_test, Y_test = data_tfidf[test_index], Y.iloc[test_index]

    # Fitting NBC
    nbc_clf.fit(X_train, Y_train)
    Y_pred = nbc_clf.predict(X_test)

    # Calculating accuracy
    score = metrics.accuracy_score(Y_test, Y_pred)
    print("Cross-validation accuracy: ", score)

    # appending cross-validation accuracy for each iteration
    scores.append(score)

nbc_mean_accuracy = np.mean(scores)
print("Mean cross-validation accuracy: ", nbc_mean_accuracy)